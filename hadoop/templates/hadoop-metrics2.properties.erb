#File managed by puppet

# syntax: [prefix].[source|sink].[instance].[options]
# See javadoc of package-info.java for org.apache.hadoop.metrics2 for details

#*.sink.file.class=org.apache.hadoop.metrics2.sink.FileSink
# default sampling period
#*.period=10

*.sink.ganglia.class=org.apache.hadoop.metrics2.sink.ganglia.GangliaSink31
*.period=10
namenode.sink.ganglia.servers=<%= ganglia_server %>:8649
datanode.sink.ganglia.servers=<%= ganglia_server %>:8649
nodemanager.sink.ganglia.servers=<%= ganglia_server %>:8649
resourcemanager.sink.ganglia.servers=<%= ganglia_server %>:8649
secondarynamenode.sink.ganglia.servers=<%= ganglia_server %>:8649
jobtracker.sink.ganglia.servers=<%= ganglia_server %>:8649
tasktracker.sink.ganglia.servers=<%= ganglia_server %>:8649

# The namenode-metrics.out will contain metrics from all context
#namenode.sink.file.filename=namenode-metrics.out
# Specifying a special sampling period for namenode:
#namenode.sink.*.period=8

#datanode.sink.file.filename=datanode-metrics.out

# the following example split metrics of different
# context to different sinks (in this case files)
#jobtracker.sink.file_jvm.context=jvm
#jobtracker.sink.file_jvm.filename=jobtracker-jvm-metrics.out
#jobtracker.sink.file_mapred.context=mapred
#jobtracker.sink.file_mapred.filename=jobtracker-mapred-metrics.out

#tasktracker.sink.file.filename=tasktracker-metrics.out

#maptask.sink.file.filename=maptask-metrics.out

#reducetask.sink.file.filename=reducetask-metrics.out
