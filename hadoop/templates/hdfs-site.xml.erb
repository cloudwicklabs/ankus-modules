<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--File Managed by puppet -->
<% @namenode_hosts = @hadoop_namenode_host.to_a -%>
<configuration>
<% if @ha != "disabled" -%>
  <!-- HA Configuration -->
  <property>
    <name>dfs.ha.automatic-failover.enabled</name>
    <value>true</value>
  </property>
  <property>
    <name>ha.zookeeper.quorum</name>
    <value><%= @zk %></value>
  </property>
  <property>
    <name>dfs.nameservices</name>
    <value><%= @hadoop_ha_nameservice_id %></value>
  </property>
  <property>
    <name>dfs.ha.namenodes.<%= @hadoop_ha_nameservice_id %></name>
    <value><%= (1..@namenode_hosts.length).map { |n| "nn#{n}" }.join(",") %></value>
  </property>
<%   @namenode_hosts.each_with_index do |host,idx| -%>
  <property>
    <name>dfs.namenode.rpc-address.<%= @hadoop_ha_nameservice_id %>.nn<%= idx+1 %></name>
    <value><%= host %>:<%= @hadoop_namenode_port %></value>
  </property>
  <property>
    <name>dfs.namenode.http-address.<%= @hadoop_ha_nameservice_id %>.nn<%= idx+1 %></name>
    <value><%= host %>:50070</value>
  </property>
<%   end -%>
  <!--Journal nodes configuration -->
  <property>
    <name>dfs.namenode.shared.edits.dir</name>
    <value><%= @journal_shared_edits_dir %></value>
  </property>
  <property>
    <name>dfs.journalnode.edits.dir</name>
    <value><%= @jn_data_dir %></value>
  </property>
  <property>
    <name>dfs.client.failover.proxy.provider.<%= @hadoop_ha_nameservice_id %></name>
    <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
  </property>
  <property>
    <name>dfs.ha.fencing.methods</name>
    <value>sshfence(<%= @sshfence_user %>)</value>
  </property>
  <property>
    <name>dfs.ha.fencing.ssh.private-key-files</name>
    <value><%= @sshfence_keypath %></value>
  </property>
<% end -%>
<% if @hadoop_security_authentication == "kerberos" -%>
  <!--Security configuraiton-->
  <property>
    <name>dfs.block.access.token.enable</name>
    <value>true</value>
  </property>
  <property>
    <name>dfs.namenode.keytab.file</name>
    <value>/etc/hdfs.keytab</value>
  </property>
  <property>
    <name>dfs.namenode.kerberos.principal</name>
    <value>hdfs/_HOST@<%= @kerberos_realm %></value>
  </property>
  <property>
    <name>dfs.namenode.kerberos.internal.spnego.principal</name>
    <value>HTTP/_HOST@<%= @kerberos_realm %></value>
  </property>
  <% if ha == "disabled" -%>
  <!-- Secondary NameNode security config -->
  <property>
    <name>dfs.secondary.namenode.keytab.file</name>
    <value>/etc/hdfs.keytab</value> <!-- path to the HDFS keytab -->
  </property>
  <property>
    <name>dfs.secondary.namenode.kerberos.principal</name>
    <value>hdfs/_HOST@<%= @kerberos_realm %></value>
  </property>
  <property>
    <name>dfs.secondary.namenode.kerberos.internal.spnego.principal</name>
    <value>HTTP/_HOST@<%= @kerberos_realm %></value>
  </property>
  <% else -%>
 <!--Quoram based storage security config -->
  <property>
    <name>dfs.journalnode.keytab.file</name>
    <value>/etc/hdfs.keytab</value>
  </property>
  <property>
    <name>dfs.journalnode.kerberos.principal</name>
    <value>hdfs/_HOST@<%= @kerberos_realm %></value>
  </property>
  <property>
    <name>dfs.journalnode.kerberos.internal.spnego.principal</name>
    <value>HTTP/_HOST@<%= @kerberos_realm %></value>
  </property>
  <%   end -%>
  <!-- DataNode security config -->
  <property>
    <name>dfs.datanode.data.dir.perm</name>
    <value>700</value>
  </property>
  <property>
    <name>dfs.datanode.address</name>
    <value>0.0.0.0:1004</value>
  </property>
  <property>
    <name>dfs.datanode.http.address</name>
    <value>0.0.0.0:1006</value>
  </property>
  <property>
    <name>dfs.datanode.keytab.file</name>
    <value>/etc/hdfs.keytab</value> <!-- path to the HDFS keytab -->
  </property>
  <property>
    <name>dfs.datanode.kerberos.principal</name>
    <value>hdfs/_HOST@<%= @kerberos_realm %></value>
  </property>
<% end -%>
  <!-- General -->
  <property>
    <name>dfs.namenode.name.dir</name>
    <value><%= @namenode_data_dirs.join(",") %></value>
  </property>
  <property>
    <name>dfs.datanode.data.dir</name>
    <value><%= @hdfs_data_dirs.join(",") %></value>
  </property>
<% if @hdfs_support_append == "true" -%>
  <property>
    <name>dfs.support.append</name>
    <value><%= @hdfs_support_append %></value>
  </property>
<% end -%>
<% if @ha == "disabled" and @hadoop_secondarynamenode_host -%>
  <property>
     <name>dfs.namenode.checkpoint.dir</name>
     <value><%= @checkpoint_data_dirs.join(",") %></value>
  </property>
  <property>
     <name>dfs.secondary.http.address</name>
     <value><%= @hadoop_secondarynamenode_host -%>:50090</value>
  </property>
<% end -%>
  <property>
    <name>dfs.permissions.superusergroup</name>
    <value><%= @hadoop_config_dfs_permissions_supergroup -%></value>
  </property>
  <property>
    <name>dfs.datanode.max.transfer.threads</name>
    <value><%= @hadoop_config_dfs_datanode_max_transfer_threads -%></value>
  </property>
  <property>
    <name>dfs.blocksize</name>
    <value><%= @hadoop_config_dfs_block_size %></value>
  </property>
  <% if @num_of_nodes > 1 -%>
  <property>
    <name>dfs.namenode.handler.count</name>
    <value><%= [Math.log(@num_of_nodes) * 20].max.round %></value>
  </property>
  <% else -%>
  <property>
    <name>dfs.namenode.handler.count</name>
    <value>10</value>
  </property>
  <% end -%>
  <property>
    <name>dfs.datanode.du.reserved</name>
    <value><%= @hadoop_config_dfs_datanode_du_reserved -%></value>
  </property>
  <property>
    <name>dfs.datanode.balance.bandwidthPerSec</name>
    <value><%= @hadoop_config_dfs_datanode_balance_bandwidthpersec -%></value>
  </property>
  <property>
    <name>dfs.permissions.enabled</name>
    <value><%= @hadoop_config_dfs_permissions_enabled -%></value>
  </property>
  <property>
    <name>dfs.namenode.safemode.threshold-pct</name>
    <value><%= @hadoop_config_dfs_namenode_safemode_threshold_pct -%></value>
  </property>
  <property>
    <name>dfs.replication</name>
    <value><%= @hadoop_config_dfs_replication -%></value>
  </property>
  <property>
    <name>dfs.namenode.replication.min</name>
    <value><%= @hadoop_config_dfs_namenode_replication_min -%></value>
  </property>
  <property>
    <name>dfs.namenode.safemode.extension</name>
    <value><%= @hadoop_config_dfs_namenode_safemode_extension -%></value>
  </property>
  <property>
    <name>dfs.df.interval</name>
    <value><%= @hadoop_config_dfs_df_interval -%></value>
  </property>
  <property>
    <name>dfs.client.block.write.retries</name>
    <value><%= @hadoop_config_dfs_client_block_write_retries -%></value>
  </property>
  <% if @hue == "enabled" -%>
  <!-- Hue settings -->
  <property>
    <name>dfs.webhdfs.enabled</name>
    <value>true</value>
    <description>enables webhdfs in namenode and datanodes</description>
  </property>
  <% if @hadoop_security_authentication == "kerberos" -%>
  <!-- WebHDFS Security settings -->
  <property>
    <name>dfs.web.authentication.kerberos.principal</name>
    <value>HTTP/_HOST@<%= @kerberos_realm %></value>
  </property>
  <property>
    <name>dfs.web.authentication.kerberos.keytab</name>
    <value>/etc/hdfs.keytab</value>
  </property>
  <% end -%>
  <% end -%>
  <% if @hbase == 'enabled' -%>
  <property>
    <name>dfs.datanode.max.xcievers</name>
    <value>4096</value>
  </property>
  <% end -%>
  <% if @impala == "enabled" -%>
  <!-- Impala Config -->
  <property>
    <name>dfs.client.read.shortcircuit</name>
    <value>true</value>
  </property>
  <property>
    <name>dfs.domain.socket.path</name>
    <value>/var/run/hadoop-hdfs/dn._PORT</value>
  </property>
  <property>
    <name>dfs.client.file-block-storage-locations.timeout</name>
    <value>3000</value>
  </property>
  <property>
    <name>dfs.datanode.hdfs-blocks-metadata.enabled</name>
    <value>true</value>
  </property>   
  <% end -%>
</configuration>